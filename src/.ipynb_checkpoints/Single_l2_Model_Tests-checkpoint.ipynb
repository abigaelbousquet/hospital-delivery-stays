{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "393a8554-7021-4480-800b-e1a580907e81",
   "metadata": {},
   "source": [
    "## Debugging by testing l2 models without cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd27404e-4476-4f4d-abae-8ec6134b2f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from math import ceil\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4cd6122-9948-487d-8797-c72b24ac92c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying data types for the columns to maintain formatting from original data\n",
    "data_types = {\n",
    "    'hospital_service_area': object, \n",
    "    'hospital_county': object,\n",
    "    'operating_certificate_number': object, \n",
    "    'permanent_facility_id': object,\n",
    "    'facility_name': object, \n",
    "    'age_group': object, \n",
    "    'zip_code_3_digits': object, \n",
    "    'gender': object, \n",
    "    'race': object,\n",
    "    'ethnicity': object, \n",
    "    'payment_typology_1': object, \n",
    "    'payment_typology_2': object,\n",
    "    'payment_typology_3': object, \n",
    "    'length_of_stay': int\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256b9287-2487-427e-b8a6-40ae803a4821",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_visits = pd.read_csv('../data/planned_deliveries.csv', dtype=data_types)\n",
    "all_visits = all_visits.loc[:, all_visits.columns != 'Unnamed: 0']\n",
    "y = all_visits['length_of_stay']\n",
    "X = all_visits.loc[:, all_visits.columns != 'length_of_stay']\n",
    "X = X.loc[:, X.columns != 'operating_certificate_number']\n",
    "X = X.loc[:, X.columns != 'facility_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a8962d6-8738-4e1d-a308-d79b7c07c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_continuous_split(X:pd.DataFrame, y:pd.Series, train_size:float, val_size:float, test_size:float, random_state:int):\n",
    "    '''\n",
    "    Performs a stratified split of inputted data (with respect to y) into a training set, validation set, and test set to specified percentages \n",
    "    of the data using verstack's scsplit and performs basic error checking.\n",
    "\n",
    "    Parameters:\n",
    "    - X: a 2D pandas DataFrame, the feature matrix\n",
    "    - y: a 1D pandas Series, the target variable matrix matching X\n",
    "    - train_size: a float between 0 and 1, the percentage of X which should be training data\n",
    "    - val_size: a float between 0 and 1, the percentage of X which should be reserved for validation\n",
    "    - test_size: a float between 0 and 1, the percentage of X which should be reserved for final testing\n",
    "    - random_state: an int, the random state to split with\n",
    "    Note: The sum of train_size + val_size + test_size must be 1.0 (100% of X).\n",
    "\n",
    "    Returns:\n",
    "    - (X_train) a 2D pandas DataFrame, the feature matrix of training data\n",
    "    - (y_train) a 1D pandas Series, the target variable matrix for training data\n",
    "    - (X_val) a 2D pandas DataFrame, the feature matrix of validation data\n",
    "    - (y_val) a 1D pandas Series, the target variable matrix for validation data\n",
    "    - (X_test) a 2D pandas DataFrame, the feature matrix of testing data\n",
    "    - (y_test) a 1D pandas Series, the target variable matrix for testing data\n",
    "\n",
    "    Raises:\n",
    "    - ValueError for invalid input\n",
    "    '''\n",
    "    from verstack.stratified_continuous_split import scsplit\n",
    "    \n",
    "    if ((train_size + val_size + test_size) != 1):\n",
    "        raise ValueError('Your train_size + val_size + test_size must add up to 1 (100%)!')\n",
    "    if (not isinstance(random_state, int)):\n",
    "        raise ValueError('Your random_state must be an int!')\n",
    "\n",
    "    X_train, X_other, y_train, y_other = scsplit(X, y, stratify=y, test_size=(1-train_size), random_state=random_state)\n",
    "    \n",
    "    X_len = X.shape[0]\n",
    "    test_percent_of_other = (test_size * X_len)/(X_len - (train_size * X_len))\n",
    "    X_other = X_other.reset_index(drop=True)\n",
    "    y_other = y_other.reset_index(drop=True)\n",
    "    \n",
    "    X_val, X_test, y_val, y_test = scsplit(X_other, y_other, stratify=y_other, test_size=test_percent_of_other, random_state=random_state)\n",
    "\n",
    "    # basic error checking to check that split returned train, val, and test of expected sizes\n",
    "    train_count_low = (int)(train_size * X_len)\n",
    "    train_count_high = ceil(train_size * X_len)\n",
    "    val_count_low = (int)(val_size * X_len)\n",
    "    val_count_high = ceil(val_size * X_len)\n",
    "    test_count_low = (int)(test_size * X_len)\n",
    "    test_count_high = ceil(test_size * X_len)\n",
    "    \n",
    "    Xtrain_fin = X_train.shape[0]\n",
    "    ytrain_fin = y_train.shape[0]\n",
    "    Xval_fin = X_val.shape[0]\n",
    "    yval_fin = y_val.shape[0]\n",
    "    Xtest_fin = X_test.shape[0]\n",
    "    ytest_fin = y_test.shape[0]\n",
    "    \n",
    "    if not (((Xtrain_fin == train_count_low) or (Xtrain_fin == train_count_high)) and ((ytrain_fin == train_count_low) or (ytrain_fin == train_count_high))):\n",
    "        raise ValueError(f'Training set size should be approx. {train_size * X_len}, instead is: {X_train.shape[0]}')\n",
    "    if not (((Xval_fin == val_count_low) or (Xval_fin == val_count_high)) and ((yval_fin == val_count_low) or (yval_fin == val_count_high))):\n",
    "        raise ValueError(f'Validation set size should be approx. {val_size * X_len}, instead is: {X_val.shape[0]}')\n",
    "    if not (((Xtest_fin == test_count_low) or (Xtest_fin == test_count_high)) and ((ytest_fin == test_count_low) or (ytest_fin == test_count_high))):\n",
    "        raise ValueError(f'Test set size should be approx. {test_size * X_len}, instead is: {X_test.shape[0]}')\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead32cd3-076b-4ca6-83f0-85e159abe38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_non_nans(X, ftr):\n",
    "    '''\n",
    "    Determines the unique, non-nan values of a certain feature in a feature matrix.\n",
    "    Does not error check that ftr is in fact a column in X; this is the responsibility of the caller.\n",
    "    \n",
    "    - X: a 2D DataFrame feature matrix containing the feature of interest as a column\n",
    "    - ftr: a string, the name of the feature to explore in X\n",
    "    \n",
    "    Returns:\n",
    "    - (unique_vals_nonull) a list of the unique, non-null values of ftr in X, replacing a nan value with 'not reported'\n",
    "    '''\n",
    "    unique_vals = X[ftr].unique()\n",
    "    unique_vals_nonull = ['not reported' if isinstance(x, float) and np.isnan(x) else x for x in unique_vals]\n",
    "\n",
    "    return unique_vals_nonull    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daeb9831-2255-4483-8e96-5a180aabb543",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "# categorizing the columns in my dataset by how they should be encoded\n",
    "onehot_ftrs = ['hospital_service_area', 'hospital_county', 'permanent_facility_id', \\\n",
    "               'zip_code_3_digits', 'gender', 'race', 'ethnicity', 'payment_typology_1', \\\n",
    "               'payment_typology_2', 'payment_typology_3']\n",
    "onehot_cats = [unique_non_nans(X, ftr) for ftr in onehot_ftrs]\n",
    "ordinal_ftrs = ['age_group']\n",
    "ordinal_cats = [['0 to 17', '18 to 29', '30 to 49', '50 to 69', '70 or Older']]\n",
    "\n",
    "# replace missing values in categorical columns with 'not reported'\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='not reported')),\n",
    "    ('onehot', OneHotEncoder(categories=onehot_cats, sparse=False, handle_unknown='ignore'))])\n",
    "\n",
    "# my data has no missing values in its ordinal column, so only encoding is necessary\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('ordinal', OrdinalEncoder(categories = ordinal_cats))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', categorical_transformer, onehot_ftrs),\n",
    "        ('ordinal', ordinal_transformer, ordinal_ftrs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ff15f36-bc6c-49fa-9349-dbac28e8655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = stratified_continuous_split(X, y, train_size=0.6, val_size=0.2, test_size=0.2, random_state=28)\n",
    "\n",
    "# preprocess\n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "X_val_prep = preprocessor.transform(X_val)\n",
    "X_test_prep = preprocessor.transform(X_test)\n",
    "prep_1_ftrs = preprocessor.get_feature_names_out()\n",
    "X_train_prep = pd.DataFrame(X_train_prep, columns=prep_1_ftrs)\n",
    "X_val_prep = pd.DataFrame(X_val_prep, columns=prep_1_ftrs)\n",
    "X_test_prep = pd.DataFrame(X_test_prep, columns=prep_1_ftrs)\n",
    "\n",
    "# final preprocess with Standard Scaler so that I can use the coefficients of linear models as global importance metrics\n",
    "standard_scaler_transformer = Pipeline(steps=[\n",
    "    ('std', StandardScaler())])\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    transformers=[('std', standard_scaler_transformer, preprocessor.get_feature_names_out())])\n",
    "\n",
    "X_train_prep = final_preprocessor.fit_transform(X_train_prep)\n",
    "X_val_prep = final_preprocessor.transform(X_val_prep)\n",
    "X_test_prep = final_preprocessor.transform(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db30b95-68a5-46b2-8446-a54056d5d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_l2 = Ridge(random_state=random_state, )\n",
    "clf = ML_algo.fit(X_train_prep, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
